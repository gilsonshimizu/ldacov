---
title: "ldacov - A new LDA formulation with covariates"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ldacov - A new LDA formulation with covariates}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<style>
body {
text-align: justify}
</style>


ldacov (@shimizu2020ldacov) is an R package that estimates an LDA model with covariates using MCMC algorithms.
The model implemented in this package uses the number of elements in each cluster as the response variable. The logarithmic link function allows an easy interpretation of the regression coefficients that link covariates to the response.


## Installation
Use the devtools package to install ldacov directly from github.
```
install_github("gilsonshimizu/ldacov")
```
The package can then be loaded using

```{r setup}
library(ldacov)
```

## Datasets

To run an LDA model with covariates, we need a counting matrix $y$ with dimension $l\times s$ ($l$ is the number of experimental units, and $s$ the number of categories). In text analysis, for instance, 
$y$ is the bag-of-words matrix, where $l$ is the number of text documents and $s$ is the number of tokens (words).

We also need an matrix $X$ of dimension $l \times d$ with the values for each of the $d$ covariates for each experimental unit.

Here we use a simulated dataset which contains both matrices $y$ and $X$:

```{r}
data("sim_data")
```

## Optimal number of clusters

We start by finding the optimal number of clusters. Although this quantity could be chosen *a priori*, here  we use an LDA model without covariates that uses the TSB (truncated stick-breaking) prior (@albuquerque2019bayesian). This model is implemented using the function `gibbs.LDA`.


```{r echo=T, results='hide'}
lda_no_covariates=gibbs.LDA(y=sim_data$y,
                            ncomm=10,
                            ngibbs=1000,
                            nburn=500,
                            psi=0.01,
                            gamma=0.1)
```


```{r}
array.lsk.init=lda_no_covariates$array.lsk
nlk=apply(array.lsk.init,c(1,3),sum)
theta=nlk/apply(nlk,1,sum)
cumsum(colSums(theta,na.rm=TRUE)/nrow(sim_data$y))
```

On this dataset,
4 clusters contain more than 99% of the elements. Thus, we choose to use 4 clusters in the following analysis.

## Estimation
Once the number of clusters has been defined,
the estimation of the parameters of ldacov are made through the function `gibbs.LDA.cov`. This function generates a chain of the parameters  $\Phi$ and $\Theta$, as well as the regression coefficients.
If `estimate.phi=FALSE` then $\Phi$ will not be estimated and the matrix containing the posterior sample (`phi.init`) of the model without covariates will be used. If `estimate.phi=TRUE` $\Phi$ will be estimated and the matrix containing the posterior sample (`phi.init`) of the model without covariates will be used only to initialize the model.

```{r echo=T, results='hide'}
lda_with_covariates <- gibbs.LDA.cov(ncomm=4,
                                     ngibbs=1000,
                                     nburn=500,
                                     y=sim_data$y,
                                     xmat=sim_data$xmat,
                                     phi.prior=0.01,
                                     array.lsk.init=lda_no_covariates$array.lsk,
                                     var.betas=c(10,rep(10,ncol(sim_data$xmat)-1)),
                                     phi.init=lda_no_covariates$phi,
                                     estimate.phi=FALSE)
```

The convergence of the model can be checked using

```{r fig.width=7,fig.height=7}
plot(lda_with_covariates$llk,type='l',xlab="iterations",ylab='log-likelihood')
```



Typically, the main goals of the ldacov  model are: (i) to verify which covariates explain the quantities in each cluster, (ii) to understand the category distributions in each cluster (matrix $\Phi$), (iii) and understand the distribution of the clusters within each experimental unit (matrix $\Theta$). 

We can see  the estimated matrices $\Theta$ and $\Phi$ using
```{r}
library(ggplot2)
seq=seq(from=500,to=1000,by=1)
lda_with_covariates.theta <- matrix(colMeans(lda_with_covariates$nlk[seq,]),nrow=nrow(sim_data$y),ncol=4)/rowSums(matrix(colMeans(lda_with_covariates$nlk[seq,]),nrow=nrow(sim_data$y),ncol=4))

data <- expand.grid(X=1:nrow(sim_data$y), Y=1:4)
data$Z <- as.vector(lda_with_covariates.theta)
```

```{r  fig.width=7,fig.height=7}
ggplot(data, aes(X, Y, fill= Z)) + geom_tile() + 
  scale_fill_gradient(low="green", high="darkblue",na.value="green") +
  labs(x = "Sample units") + labs(y = "Cluster distribution (%)") +
  labs(fill = " ")+theme_minimal() +
  ggtitle("Estimated Theta matrix") 
```
```{r}
lda_with_covariates.phi <- matrix(colMeans(lda_with_covariates$phi[seq,]),nrow=4,ncol=ncol(sim_data$y))
data <- expand.grid(X=1:ncol(sim_data$y), Y=1:4)
data$Z <- as.vector(lda_with_covariates.phi)
```
```{r fig.width=7,fig.height=7}
ggplot(data, aes(X, Y, fill= Z)) + geom_tile() + 
  scale_fill_gradient(low="green", high="darkblue",na.value="green") +
  labs(x = "Category distribution (%)") + labs(y = "Cluster") +
  labs(fill = " ")+theme_minimal()+
  ggtitle("Estimated Phi matrix") 
```


In some cases (as for instance in text analysis), it may be useful  to know the main cluster categories (or main words for each topic). We define a category to be relevant for a given cluster if it appears at least twice more in this cluster than in any other cluster. This can be done as follows:

```{r}
phi=matrix(colMeans(lda_with_covariates$phi[seq,]),nrow=4,ncol=ncol(sim_data$y))
phi.max=matrix(NA,nrow=4,ncol=ncol(sim_data$y))
for (i in 1:4){phi.max[i,]=apply(phi[-i,], 2, max)}
colnames(phi.max)=1:ncol(sim_data$y)
topic_word_max=apply(phi/phi.max, 1, function(x) names(sort(x,decreasing=TRUE)))
colnames(topic_word_max)=c("Cluster 1","Cluster 2","Cluster 3","Cluster 4")
```



```{r echo = FALSE, results = 'asis'}
library(knitr)
kable(topic_word_max[1:10, ])
```


The coefficients of the regression can be extracted using

```{r}
library(reshape2)
library(ggridges)

number <- 1:length(seq)
aux1 <- data.frame(number,lda_with_covariates$betas[seq,])
aux2 <- melt(aux1, id=c("number"))
aux2$variable <- (gsub("X", "", aux2$variable))
```
```{r fig.width=7,fig.height=7}
ggplot(aux2, aes(x = value, y = variable, fill = variable)) +
  geom_density_ridges(size=0.4) +
  theme_ridges() + 
  theme(legend.position = "none") +
  xlab("Value") +
  ylab("Betas") +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.x=element_blank())
```


## References
